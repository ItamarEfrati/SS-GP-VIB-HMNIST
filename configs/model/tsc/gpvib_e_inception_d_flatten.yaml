_target_: src.models.basics.ss_gp_vib.SemiSupervisedGPVIB
_partial_: True

timeseries_encoder:
  _target_: src.models.encoders.InceptionEncoder
  _partial_: True
  input_n_channels: -1
  encoding_size: 32 # how many series in the lattent space
  encoding_series_size: -1 # the length of each series in the latent space
  number_of_filters: 32 # there are 4 kernels so final channel size is 128
  bottleneck_size: 32
  use_bottleneck: true
  depth: 1


encoder:
  _target_: src.models.encoders.BandedJointEncoder
  _partial_: True
  precision_activation:
    _target_: torch.nn.Softmax
  encoding_size: ${model.timeseries_encoder.encoding_size}


decoder:
  _target_: src.models.decoders.FlattenMultinomialDecoder
  _partial_: True
  z_dim: ${model.timeseries_encoder.encoding_size}
  hidden_size_1: -1
  hidden_size_2: -1
  hidden_size_3: -1
  num_samples: ${model.num_samples}
  output_size: ${model.num_classes}


data_decoder:
  _target_: src.models.decoders.GaussianDataDecoder
  _partial_: True
  output_n_channels:
  output_length:
  latent_n_channels:
  latent_length:
  num_samples: ${model.num_samples}


is_vae: false
data_beta: 0
entropy_coef: 0
reconstruction_coef: 0

optimizer:
  _target_: torch.optim.Adam
  _partial_: true
  lr: 0.001
  weight_decay: 0.005

scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _partial_: true
  factor: 0.5
  patience: 50
  mode: ${callbacks.early_stopping.mode}
  threshold: 0.00001
  cooldown: 0
  min_lr: 1e-5

# vib
num_classes: -1
alpha: -1
beta: 0.01
num_samples: 1
class_weight_file:
monitor_metric: ${callbacks.early_stopping.monitor} # for the scheduler
use_class_weight: false
sample_during_evaluation: false

# time series vib
is_ssl: false

# gp_vib
kernel: cauchy
sigma: 0.7
length_scale: 1
kernel_scales: 1